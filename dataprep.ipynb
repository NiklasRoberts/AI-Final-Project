{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tree measurements for year 82, 85, and 90\n",
    "import pandas as pd\n",
    "treeMeasurements = pd.read_csv(\"C55_TreeMeasurements.csv\")\n",
    "\n",
    "#getting trees from 1982 and 1985 measurements\n",
    "desiredCols1 = [\"PLOT\", \"TREE\", \"RECRUIT\", \"SPECIES\", \"D82\", \"D85\"]\n",
    "customMeasurements1 = treeMeasurements[desiredCols1]\n",
    "\n",
    "#getting trees from 1985 to 1990\n",
    "desiredCols2 = [\"PLOT\", \"TREE\", \"RECRUIT\", \"SPECIES\", \"D85\", \"D90\"]\n",
    "customMeasurements2 = treeMeasurements[desiredCols2]\n",
    "\n",
    "#cleaning data to gather trees that only have measurements at both ends of the period\n",
    "clean82_85 = customMeasurements1.dropna()\n",
    "clean85_90 = customMeasurements2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting foliar nutrients from csv file\n",
    "foliarNutrients = pd.read_csv(\"C55_FoliarNutrients.csv\")\n",
    "foliarNutrients = foliarNutrients.drop(\"REPLICATE\", axis=1)\n",
    "\n",
    "# Get foliar nutrient levels 1982\n",
    "foliarNutrients82 = foliarNutrients.loc[foliarNutrients['YEAR'] == 1982]\n",
    "\n",
    "# Get foliar nutrient levels 1985\n",
    "foliarNutrients85 = foliarNutrients.loc[foliarNutrients['YEAR'] == 1985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add foliar nutrients to tree data \n",
    "allTreeData82_85 = pd.merge(clean82_85, foliarNutrients82, on=\"PLOT\", how=\"outer\")\n",
    "allTreeData85_90 = pd.merge(clean85_90, foliarNutrients85, on=\"PLOT\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting percent growth for each tree per year\n",
    "# perc_growth = (D85-D82)/D82\n",
    "allTreeData82_85[\"Growth\"] = (allTreeData82_85[\"D85\"]-allTreeData82_85[\"D82\"])/allTreeData82_85[\"D82\"]\n",
    "# growth_per_yr = prec_growth/85-82\n",
    "allTreeData82_85[\"Growth/yr\"] = allTreeData82_85[\"Growth\"]/3\n",
    "\n",
    "#getting percent growth for 1985-1990\n",
    "# perc_growth = (D90-D85)/D825\n",
    "allTreeData85_90[\"Growth\"] = (allTreeData85_90[\"D90\"]-allTreeData85_90[\"D85\"])/allTreeData85_90[\"D85\"]\n",
    "# growth_per_yr = prec_growth/85-82\n",
    "allTreeData85_90[\"Growth/yr\"] = allTreeData85_90[\"Growth\"]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns: \n",
    "relColumns = [\"SPECIES\", \"TOTN\", \"TOTP\", \"TOTK\", \"TOTS\", \"TOTCa\", \"Growth/yr\"]\n",
    "modelingData82 = allTreeData82_85[relColumns]\n",
    "modelingData85 = allTreeData85_90[relColumns]\n",
    "\n",
    "# combine dataframes to make one dataframe with all data needed\n",
    "modelingData = pd.concat([modelingData82, modelingData85], ignore_index=True)\n",
    "modelingData['id'] = modelingData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either jump to regression, or keep going to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='bins'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimental data analysis looking at distribution and possible binning\n",
    "import matplotlib\n",
    "\n",
    "# creating a copy of current modeling data\n",
    "modelingCopy = modelingData\n",
    "\n",
    "# Creating Histogram for percent growth per year\n",
    "bins = [0, .05, .1, .2, 50]\n",
    "modelingCopy['bins'] = pd.cut(modelingData['Growth/yr'], bins = bins, include_lowest=True)\n",
    "plotDF = modelingCopy.groupby('bins').bins.count()\n",
    "plotDF.plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Columns for neural network classification\n",
    "# What bin does the growth/yr fall into? that bin gets a 1, others get 0\n",
    "\n",
    "mergeDF = pd.DataFrame(columns=['id', 'bin'])\n",
    "\n",
    "# For each row in modelingData setting a bin based off how growth/year\n",
    "for index, row in modelingData.iterrows():\n",
    "    #bins: 0-0.05, 0.05+ to 0.1, 0.1+ to 0.2, 0.2+\n",
    "    if row['Growth/yr'] <= .05:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 0})\n",
    "    elif row['Growth/yr'] <= .1:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 1})\n",
    "    elif row['Growth/yr'] <= .2:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 2})\n",
    "    else:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 3})\n",
    "\n",
    "# joining data to include bin\n",
    "modelingData = pd.merge(modelingData, mergeDF, on='id', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-3f59790ad1d2>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  classifyData['SPECIES'] = speciesNumList\n"
     ]
    }
   ],
   "source": [
    "# Getting data ready for the Classification Neural Network\n",
    "\n",
    "# Input variables: SPECIES, TOTN, TOTP, TOTK, TOTS, TOTCa, and TOTMg (not used)\n",
    "# label: fixed bins (1 for correct bin, 0 for all others)\n",
    "classifyColumns = ['SPECIES', 'TOTN', 'TOTP', 'TOTK', 'TOTS', 'TOTCa', 'bin']\n",
    "classifyData = modelingData[classifyColumns]\n",
    "\n",
    "# for turning 'species' column into numerical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "speciesLabEncoder = LabelEncoder()\n",
    "\n",
    "# Transforms species into numerical data\n",
    "speciesNumList = speciesLabEncoder.fit_transform(classifyData['SPECIES'])\n",
    "# code for reverting: species = le.inverse_transform(label)\n",
    "# changing the species column to numerical values\n",
    "classifyData['SPECIES'] = speciesNumList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into input and output data\n",
    "classifyX = classifyData.iloc[:,:6].values\n",
    "classifyY = classifyData.iloc[:,6:7].values\n",
    "\n",
    "# Transforming output into binary values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "classifyY = ohe.fit_transform(classifyY).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "classifyX_train, classifyX_test, classifyY_train, classifyY_test = train_test_split(classifyX, classifyY, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building and Compiling Neural Network for Classification\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifyModel = Sequential()\n",
    "classifyModel.add(Dense(24, input_dim=6, activation='relu'))\n",
    "classifyModel.add(Dense(24, activation='relu'))\n",
    "#trying to overfit\n",
    "classifyModel.add(Dense(36, activation='relu'))\n",
    "classifyModel.add(Dense(36, activation='relu'))\n",
    "classifyModel.add(Dense(24, activation='relu'))\n",
    "classifyModel.add(Dense(12, activation='relu'))\n",
    "classifyModel.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "classifyModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 2s 2ms/step - loss: 1.3282 - accuracy: 0.4311\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.1120 - accuracy: 0.4983\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0628 - accuracy: 0.5002\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1.0326 - accuracy: 0.5192\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0210 - accuracy: 0.5301\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0178 - accuracy: 0.5154\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0086 - accuracy: 0.5251\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 1.0062 - accuracy: 0.5239\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - ETA: 0s - loss: 1.0110 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0105 - accuracy: 0.5332\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0097 - accuracy: 0.5318\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0083 - accuracy: 0.5276\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0079 - accuracy: 0.5220\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9951 - accuracy: 0.5286\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0004 - accuracy: 0.5238: 0s - loss: 1\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 1s 6ms/step - loss: 1.0083 - accuracy: 0.5159: 0s - loss: - ETA: 0s - loss: 1.0092 - accuracy: \n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9919 - accuracy: 0.5284\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0038 - accuracy: 0.5198: 0s - loss: 1.0053 - accura\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 1s 6ms/step - loss: 1.0064 - accuracy: 0.5164\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 0.9894 - accuracy: 0.5373: 0s - loss: 0.9846 - accuracy: 0.53 - ETA: 0s - loss: 0.9850 \n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1.0008 - accuracy: 0.5233\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9909 - accuracy: 0.5272\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9948 - accuracy: 0.5288\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9823 - accuracy: 0.5391\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9790 - accuracy: 0.5399\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9937 - accuracy: 0.5300\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9982 - accuracy: 0.5256\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9866 - accuracy: 0.5396\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9925 - accuracy: 0.5352\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9959 - accuracy: 0.5251\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9892 - accuracy: 0.5340\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9759 - accuracy: 0.5447\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9835 - accuracy: 0.5437\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9843 - accuracy: 0.5469\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9940 - accuracy: 0.5328\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9697 - accuracy: 0.5523\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5397\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9762 - accuracy: 0.5494\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9831 - accuracy: 0.5479\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5467\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9679 - accuracy: 0.5495\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9712 - accuracy: 0.5562\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9673 - accuracy: 0.5529\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.5532\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9687 - accuracy: 0.5573\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9833 - accuracy: 0.5485\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9626 - accuracy: 0.5532\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9693 - accuracy: 0.5534\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9586 - accuracy: 0.5612\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9715 - accuracy: 0.5529\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9640 - accuracy: 0.5534\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9701 - accuracy: 0.5553\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9470 - accuracy: 0.5593\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9694 - accuracy: 0.5638\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9569 - accuracy: 0.5620\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9566 - accuracy: 0.5633\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9538 - accuracy: 0.5649\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9594 - accuracy: 0.5595\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9569 - accuracy: 0.5686\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9465 - accuracy: 0.5698\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5594\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9730 - accuracy: 0.5615\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9659 - accuracy: 0.5625\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9622 - accuracy: 0.5655\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9566 - accuracy: 0.5659\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.5669\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5635\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9501 - accuracy: 0.5716\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9609 - accuracy: 0.5695\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.5637\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9545 - accuracy: 0.5709\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9663 - accuracy: 0.5602\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9543 - accuracy: 0.5654\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9638 - accuracy: 0.5681\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9514 - accuracy: 0.5682\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.5669\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9550 - accuracy: 0.5596\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9590 - accuracy: 0.5649\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9469 - accuracy: 0.5710\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9446 - accuracy: 0.5765\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9578 - accuracy: 0.5688\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9562 - accuracy: 0.5684\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9520 - accuracy: 0.5696\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9450 - accuracy: 0.5685\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9488 - accuracy: 0.5737\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9435 - accuracy: 0.5688\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9483 - accuracy: 0.5754\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9658 - accuracy: 0.5564\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9458 - accuracy: 0.5700\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9497 - accuracy: 0.5723\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9498 - accuracy: 0.5685\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9432 - accuracy: 0.5729\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9551 - accuracy: 0.5751\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9682 - accuracy: 0.5605\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9519 - accuracy: 0.5722\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9521 - accuracy: 0.5663\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9574 - accuracy: 0.5674\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9383 - accuracy: 0.5830\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9469 - accuracy: 0.5655\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9409 - accuracy: 0.5671\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "#Training classification model\n",
    "training = classifyModel.fit(classifyX_train, classifyY_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classification model\n",
    "import numpy as np\n",
    "prediction = classifyModel.predict(classifyX_test)\n",
    "\n",
    "# undoing onehot encoder\n",
    "pred = list()\n",
    "for i in range(len(prediction)):\n",
    "    pred.append(np.argmax(prediction[i]))\n",
    "test = list()\n",
    "for i in range(len(classifyY_test)):\n",
    "    test.append(np.argmax(classifyY_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.5727190605239386\n"
     ]
    }
   ],
   "source": [
    "# Determining accuracy of Classification Neural Network model\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifyAccuracy = accuracy_score(pred,test)\n",
    "print('Accuracy is:', classifyAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-43b84e18647f>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  regressData['SPECIES'] = speciesNumList\n"
     ]
    }
   ],
   "source": [
    "# Getting Columns for neural network regression\n",
    "# input variables: SPECIES, TOTN, TOTP, TOTK, TOTS, TOTCa, and TOTMg (not used)\n",
    "# label: predicting growth per year\n",
    "\n",
    "regressColumns = ['SPECIES', 'TOTN', 'TOTP', 'TOTK', 'TOTS', 'TOTCa', 'Growth/yr']\n",
    "regressData = modelingData[regressColumns]\n",
    "\n",
    "# for turning 'species' column into numerical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "speciesLabEncoder = LabelEncoder()\n",
    "\n",
    "# Transforms species into numerical data\n",
    "speciesNumList = speciesLabEncoder.fit_transform(regressData['SPECIES'])\n",
    "# code for reverting: species = le.inverse_transform(label)\n",
    "# changing the species column to numerical values\n",
    "regressData['SPECIES'] = speciesNumList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "# Splitting into input and output data\n",
    "regressX = regressData.iloc[:,:6].values\n",
    "regressY = regressData.iloc[:,6:7].values\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "regressY = np.reshape(regressY, (-1,1))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print (scaler_x.fit(regressX))\n",
    "xscale = scaler_x.transform(regressX)\n",
    "print (scaler_y.fit(regressY))\n",
    "yscale = scaler_y.transform(regressY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "regressX_train, regressX_test, regressY_train, regressY_test = train_test_split(xscale, yscale, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building and Compiling Neural Network for Regression\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "regressModel = Sequential()\n",
    "regressModel.add(Dense(12, input_dim = 6, kernel_initializer='normal', activation='relu'))\n",
    "regressModel.add(Dense(12, activation='relu'))\n",
    "regressModel.add(Dense(1, activation='linear'))\n",
    "# 6/12/12/1\n",
    "#regressModel.summary()\n",
    "loss_function = keras.losses.MeanAbsoluteError()\n",
    "regressModel.compile(loss=loss_function, optimizer='adam', metrics=['mse','mae','mape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6088e-04 - mae: 0.0099 - mape: 8212.4834\n",
      "Epoch 2/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6105e-04 - mae: 0.0098 - mape: 7944.0615\n",
      "Epoch 3/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.5965e-04 - mae: 0.0098 - mape: 8156.7031\n",
      "Epoch 4/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6089e-04 - mae: 0.0098 - mape: 8015.4580\n",
      "Epoch 5/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6468e-04 - mae: 0.0098 - mape: 8071.4922\n",
      "Epoch 6/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6109e-04 - mae: 0.0098 - mape: 7946.0601\n",
      "Epoch 7/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6062e-04 - mae: 0.0098 - mape: 8220.0664\n",
      "Epoch 8/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6151e-04 - mae: 0.0098 - mape: 8008.7148\n",
      "Epoch 9/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6032e-04 - mae: 0.0098 - mape: 8002.5410\n",
      "Epoch 10/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5954e-04 - mae: 0.0098 - mape: 8176.8438\n",
      "Epoch 11/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6065e-04 - mae: 0.0098 - mape: 7899.0728\n",
      "Epoch 12/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0099 - mse: 5.5989e-04 - mae: 0.0099 - mape: 8233.6650\n",
      "Epoch 13/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6146e-04 - mae: 0.0098 - mape: 8044.1880\n",
      "Epoch 14/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6138e-04 - mae: 0.0098 - mape: 7915.2593\n",
      "Epoch 15/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6169e-04 - mae: 0.0098 - mape: 8111.6826\n",
      "Epoch 16/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6252e-04 - mae: 0.0098 - mape: 8019.5015A: 0s - loss: 0.0097 - mse: 5.9794e-04 - mae: 0.0097 - mape: 5604\n",
      "Epoch 17/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6252e-04 - mae: 0.0098 - mape: 8145.4429\n",
      "Epoch 18/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.5999e-04 - mae: 0.0098 - mape: 8040.9775\n",
      "Epoch 19/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6048e-04 - mae: 0.0098 - mape: 8013.1157\n",
      "Epoch 20/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6221e-04 - mae: 0.0099 - mape: 7977.9507\n",
      "Epoch 21/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6122e-04 - mae: 0.0098 - mape: 8151.5674\n",
      "Epoch 22/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6124e-04 - mae: 0.0099 - mape: 7965.1030\n",
      "Epoch 23/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6211e-04 - mae: 0.0098 - mape: 7969.1206\n",
      "Epoch 24/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6213e-04 - mae: 0.0098 - mape: 7924.1074\n",
      "Epoch 25/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.5998e-04 - mae: 0.0098 - mape: 8196.5908\n",
      "Epoch 26/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6237e-04 - mae: 0.0098 - mape: 8162.2427\n",
      "Epoch 27/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5961e-04 - mae: 0.0098 - mape: 8008.5205\n",
      "Epoch 28/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.5934e-04 - mae: 0.0098 - mape: 8026.6494\n",
      "Epoch 29/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6248e-04 - mae: 0.0098 - mape: 7915.8384\n",
      "Epoch 30/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6238e-04 - mae: 0.0098 - mape: 8133.2334\n",
      "Epoch 31/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6059e-04 - mae: 0.0099 - mape: 8223.9521\n",
      "Epoch 32/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5978e-04 - mae: 0.0098 - mape: 8074.8770\n",
      "Epoch 33/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6036e-04 - mae: 0.0099 - mape: 7976.1357\n",
      "Epoch 34/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.5948e-04 - mae: 0.0099 - mape: 8166.3945\n",
      "Epoch 35/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6068e-04 - mae: 0.0098 - mape: 8221.4658\n",
      "Epoch 36/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6188e-04 - mae: 0.0098 - mape: 8235.2559\n",
      "Epoch 37/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6209e-04 - mae: 0.0098 - mape: 8027.4053\n",
      "Epoch 38/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6018e-04 - mae: 0.0098 - mape: 8096.3018\n",
      "Epoch 39/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6119e-04 - mae: 0.0098 - mape: 7996.4761\n",
      "Epoch 40/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6000e-04 - mae: 0.0098 - mape: 8114.8472\n",
      "Epoch 41/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6128e-04 - mae: 0.0098 - mape: 8177.3408\n",
      "Epoch 42/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6152e-04 - mae: 0.0098 - mape: 7963.0098\n",
      "Epoch 43/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6245e-04 - mae: 0.0098 - mape: 8139.6260\n",
      "Epoch 44/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6153e-04 - mae: 0.0098 - mape: 7926.2671A: 0s - loss: 0.0102 - mse: 9.5710e-04 - mae: 0.0102 - mape: 896\n",
      "Epoch 45/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0099 - mse: 5.6464e-04 - mae: 0.0099 - mape: 8127.1499\n",
      "Epoch 46/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6286e-04 - mae: 0.0099 - mape: 8039.8037\n",
      "Epoch 47/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5983e-04 - mae: 0.0098 - mape: 7963.7354\n",
      "Epoch 48/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5977e-04 - mae: 0.0098 - mape: 7989.7891\n",
      "Epoch 49/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6308e-04 - mae: 0.0098 - mape: 7849.9590\n",
      "Epoch 50/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6258e-04 - mae: 0.0098 - mape: 8205.3984\n",
      "Epoch 51/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6179e-04 - mae: 0.0098 - mape: 8003.1445\n",
      "Epoch 52/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6184e-04 - mae: 0.0098 - mape: 8091.2798\n",
      "Epoch 53/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5916e-04 - mae: 0.0098 - mape: 8038.0068\n",
      "Epoch 54/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6004e-04 - mae: 0.0098 - mape: 8113.6626\n",
      "Epoch 55/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 5.5978e-04 - mae: 0.0097 - mape: 7930.9629\n",
      "Epoch 56/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6136e-04 - mae: 0.0098 - mape: 8078.3916\n",
      "Epoch 57/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6324e-04 - mae: 0.0098 - mape: 7990.7974\n",
      "Epoch 58/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6031e-04 - mae: 0.0098 - mape: 8222.6611\n",
      "Epoch 59/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6404e-04 - mae: 0.0099 - mape: 8145.2632\n",
      "Epoch 60/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6419e-04 - mae: 0.0099 - mape: 8106.3364\n",
      "Epoch 61/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 5.5974e-04 - mae: 0.0097 - mape: 8074.7705\n",
      "Epoch 62/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6224e-04 - mae: 0.0098 - mape: 8026.1714\n",
      "Epoch 63/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6134e-04 - mae: 0.0098 - mape: 7981.0723\n",
      "Epoch 64/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6082e-04 - mae: 0.0098 - mape: 8128.6255\n",
      "Epoch 65/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6063e-04 - mae: 0.0098 - mape: 8185.1357\n",
      "Epoch 66/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6168e-04 - mae: 0.0098 - mape: 8149.8701\n",
      "Epoch 67/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6106e-04 - mae: 0.0098 - mape: 7918.0547\n",
      "Epoch 68/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6183e-04 - mae: 0.0098 - mape: 8061.0020\n",
      "Epoch 69/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5987e-04 - mae: 0.0098 - mape: 8084.1689\n",
      "Epoch 70/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6055e-04 - mae: 0.0098 - mape: 8039.1650\n",
      "Epoch 71/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6308e-04 - mae: 0.0098 - mape: 7944.9395\n",
      "Epoch 72/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6160e-04 - mae: 0.0098 - mape: 8145.7490\n",
      "Epoch 73/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6202e-04 - mae: 0.0098 - mape: 7907.8706\n",
      "Epoch 74/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6117e-04 - mae: 0.0098 - mape: 8036.9224\n",
      "Epoch 75/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6114e-04 - mae: 0.0098 - mape: 8003.0186\n",
      "Epoch 76/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6333e-04 - mae: 0.0098 - mape: 8043.7661\n",
      "Epoch 77/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6058e-04 - mae: 0.0098 - mape: 8073.3706\n",
      "Epoch 78/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6055e-04 - mae: 0.0098 - mape: 8135.0239\n",
      "Epoch 79/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6019e-04 - mae: 0.0098 - mape: 8075.4976\n",
      "Epoch 80/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 5.6121e-04 - mae: 0.0097 - mape: 8096.3188\n",
      "Epoch 81/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6130e-04 - mae: 0.0098 - mape: 7978.8589\n",
      "Epoch 82/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6258e-04 - mae: 0.0098 - mape: 7963.7651\n",
      "Epoch 83/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6162e-04 - mae: 0.0098 - mape: 8039.6592\n",
      "Epoch 84/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5888e-04 - mae: 0.0098 - mape: 8062.9746\n",
      "Epoch 85/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5929e-04 - mae: 0.0098 - mape: 8001.4556\n",
      "Epoch 86/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6207e-04 - mae: 0.0098 - mape: 7990.2671\n",
      "Epoch 87/150\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 5.6196e-04 - mae: 0.0098 - mape: 8162.9717\n",
      "Epoch 88/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6054e-04 - mae: 0.0099 - mape: 8137.1890\n",
      "Epoch 89/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6102e-04 - mae: 0.0098 - mape: 8084.8320\n",
      "Epoch 90/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6175e-04 - mae: 0.0098 - mape: 7916.6606\n",
      "Epoch 91/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6279e-04 - mae: 0.0098 - mape: 7984.8306\n",
      "Epoch 92/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6277e-04 - mae: 0.0098 - mape: 8154.5552\n",
      "Epoch 93/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5922e-04 - mae: 0.0098 - mape: 8052.2666\n",
      "Epoch 94/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 5.6010e-04 - mae: 0.0097 - mape: 8041.4482\n",
      "Epoch 95/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6133e-04 - mae: 0.0098 - mape: 8095.1758A: 0s - loss: 0.0098 - mse: 5.6772e-04 - mae: 0.0098 - mape: 8173.535\n",
      "Epoch 96/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 5.6036e-04 - mae: 0.0097 - mape: 8038.1704\n",
      "Epoch 97/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6139e-04 - mae: 0.0098 - mape: 7968.6733\n",
      "Epoch 98/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6015e-04 - mae: 0.0098 - mape: 7977.1465A: 0s - loss: 0.0094 - mse: 3.4708e-04 - mae: 0.0094 - mape: 8102\n",
      "Epoch 99/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.5994e-04 - mae: 0.0098 - mape: 8039.3892\n",
      "Epoch 100/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.5903e-04 - mae: 0.0098 - mape: 8231.6553\n",
      "Epoch 101/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6088e-04 - mae: 0.0099 - mape: 8023.8267\n",
      "Epoch 102/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6109e-04 - mae: 0.0098 - mape: 7927.0903\n",
      "Epoch 103/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6207e-04 - mae: 0.0098 - mape: 7967.5801\n",
      "Epoch 104/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 5.6205e-04 - mae: 0.0099 - mape: 8084.2549\n",
      "Epoch 105/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6095e-04 - mae: 0.0098 - mape: 8009.7729A: 0s - loss: 0.0099 - mse: 4.8498e-04 - mae: 0.0099 - mape: 681\n",
      "Epoch 106/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6180e-04 - mae: 0.0098 - mape: 8038.2153\n",
      "Epoch 107/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5962e-04 - mae: 0.0098 - mape: 8004.9468\n",
      "Epoch 108/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5986e-04 - mae: 0.0098 - mape: 8114.2344\n",
      "Epoch 109/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6045e-04 - mae: 0.0098 - mape: 8095.0000\n",
      "Epoch 110/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6210e-04 - mae: 0.0098 - mape: 7986.6738\n",
      "Epoch 111/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6187e-04 - mae: 0.0098 - mape: 7966.2842\n",
      "Epoch 112/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6044e-04 - mae: 0.0098 - mape: 8068.8721\n",
      "Epoch 113/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6035e-04 - mae: 0.0098 - mape: 8020.0137\n",
      "Epoch 114/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5998e-04 - mae: 0.0098 - mape: 7995.8833\n",
      "Epoch 115/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6251e-04 - mae: 0.0098 - mape: 8122.9375\n",
      "Epoch 116/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.5903e-04 - mae: 0.0098 - mape: 7991.2510\n",
      "Epoch 117/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6401e-04 - mae: 0.0098 - mape: 8001.3926\n",
      "Epoch 118/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6119e-04 - mae: 0.0098 - mape: 8145.4810\n",
      "Epoch 119/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6117e-04 - mae: 0.0098 - mape: 7973.8306\n",
      "Epoch 120/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6205e-04 - mae: 0.0098 - mape: 8117.3237\n",
      "Epoch 121/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6049e-04 - mae: 0.0098 - mape: 8013.2324A: 0s - loss: 0.0095 - mse: 3.8349e-04 - mae: 0.0095 - map\n",
      "Epoch 122/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6182e-04 - mae: 0.0098 - mape: 8089.0986\n",
      "Epoch 123/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6028e-04 - mae: 0.0098 - mape: 8032.2866\n",
      "Epoch 124/150\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0098 - mse: 5.6163e-04 - mae: 0.0098 - mape: 8152.9814\n",
      "Epoch 125/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.5899e-04 - mae: 0.0098 - mape: 8189.7627\n",
      "Epoch 126/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6033e-04 - mae: 0.0098 - mape: 8056.1924\n",
      "Epoch 127/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0097 - mse: 5.6035e-04 - mae: 0.0097 - mape: 8065.6582\n",
      "Epoch 128/150\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0099 - mse: 5.6258e-04 - mae: 0.0099 - mape: 7980.7715\n",
      "Epoch 129/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0098 - mse: 5.6094e-04 - mae: 0.0098 - mape: 7956.5908\n",
      "Epoch 130/150\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0098 - mse: 5.6128e-04 - mae: 0.0098 - mape: 7993.7275\n",
      "Epoch 131/150\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.0098 - mse: 5.6156e-04 - mae: 0.0098 - mape: 8080.6919\n",
      "Epoch 132/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6209e-04 - mae: 0.0098 - mape: 8140.3408\n",
      "Epoch 133/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0099 - mse: 5.6163e-04 - mae: 0.0099 - mape: 8042.0210\n",
      "Epoch 134/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6100e-04 - mae: 0.0098 - mape: 8011.4272\n",
      "Epoch 135/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0097 - mse: 5.5967e-04 - mae: 0.0097 - mape: 8041.0806\n",
      "Epoch 136/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6150e-04 - mae: 0.0098 - mape: 7977.3682\n",
      "Epoch 137/150\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 0.0098 - mse: 5.6206e-04 - mae: 0.0098 - mape: 8143.8716\n",
      "Epoch 138/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0097 - mse: 5.6063e-04 - mae: 0.0097 - mape: 8128.3433\n",
      "Epoch 139/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0098 - mse: 5.6024e-04 - mae: 0.0098 - mape: 7914.9482\n",
      "Epoch 140/150\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 0.0098 - mse: 5.6036e-04 - mae: 0.0098 - mape: 8037.0688\n",
      "Epoch 141/150\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.0098 - mse: 5.6144e-04 - mae: 0.0098 - mape: 7964.3643: 1s - loss: 0.0102 - mse: 7.3297e-04 - mae: 0.0102 - ETA: 0s - loss: 0.0098 - mse: 5.3608e-04 - mae: 0.0098 - mape: 7592.\n",
      "Epoch 142/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0098 - mse: 5.6070e-04 - mae: 0.0098 - mape: 7958.5103\n",
      "Epoch 143/150\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 0.0098 - mse: 5.6087e-04 - mae: 0.0098 - mape: 8116.5830: 1s - loss: 0.0097 - mse: 0.0010 - mae: 0.0097 -\n",
      "Epoch 144/150\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 0.0097 - mse: 5.5893e-04 - mae: 0.0097 - mape: 8028.4321\n",
      "Epoch 145/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6154e-04 - mae: 0.0098 - mape: 8007.2246\n",
      "Epoch 146/150\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.0098 - mse: 5.6111e-04 - mae: 0.0098 - mape: 8079.3018\n",
      "Epoch 147/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0098 - mse: 5.6111e-04 - mae: 0.0098 - mape: 8100.0488\n",
      "Epoch 148/150\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.0098 - mse: 5.6114e-04 - mae: 0.0098 - mape: 7962.7847\n",
      "Epoch 149/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6215e-04 - mae: 0.0098 - mape: 8033.7646\n",
      "Epoch 150/150\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 5.6200e-04 - mae: 0.0098 - mape: 8069.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25a84e811f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training model\n",
    "# regressY_train = regressY_train + 0.000001\n",
    "regressModel.fit(regressX_train, regressY_train, epochs=150, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  0.04062351749407305\n",
      "Max Error:  2.1302025814851127\n"
     ]
    }
   ],
   "source": [
    "prediction = regressModel.predict(regressX_test)\n",
    "pred = scaler_y.inverse_transform(prediction)\n",
    "true = scaler_y.inverse_transform(regressY_test)\n",
    "# true = true + 0.000001\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "#print(\"Mean Absolute Percentage Error: \", mape(true, pred))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "print(\"Mean Absolute Error: \", mae(true, pred))\n",
    "\n",
    "from sklearn.metrics import max_error\n",
    "print(\"Max Error: \", max_error(true, pred))\n",
    "\n",
    "absErrors = abs(true - pred)\n",
    "\n",
    "# making a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Creating Histogram for percent growth per year\n",
    "#bins = [0, .05, .1, .2, 50]\n",
    "#modelingCopy['bins'] = pd.cut(modelingData['Growth/yr'], bins = bins, include_lowest=True)\n",
    "#plotDF = modelingCopy.groupby('bins').bins.count()\n",
    "#plotDF.plot(kind=\"bar\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
