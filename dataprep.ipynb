{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tree measurements for year 82, 85, and 90\n",
    "import pandas as pd\n",
    "treeMeasurements = pd.read_csv(\"C55_TreeMeasurements.csv\")\n",
    "\n",
    "#getting trees from 1982 and 1985 measurements\n",
    "desiredCols1 = [\"PLOT\", \"TREE\", \"RECRUIT\", \"SPECIES\", \"D82\", \"D85\"]\n",
    "customMeasurements1 = treeMeasurements[desiredCols1]\n",
    "\n",
    "#getting trees from 1985 to 1990\n",
    "desiredCols2 = [\"PLOT\", \"TREE\", \"RECRUIT\", \"SPECIES\", \"D85\", \"D90\"]\n",
    "customMeasurements2 = treeMeasurements[desiredCols2]\n",
    "\n",
    "#cleaning data to gather trees that only have measurements at both ends of the period\n",
    "clean82_85 = customMeasurements1.dropna()\n",
    "clean85_90 = customMeasurements2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting foliar nutrients from csv file\n",
    "foliarNutrients = pd.read_csv(\"C55_FoliarNutrients.csv\")\n",
    "foliarNutrients = foliarNutrients.drop(\"REPLICATE\", axis=1)\n",
    "\n",
    "# Get foliar nutrient levels 1982\n",
    "foliarNutrients82 = foliarNutrients.loc[foliarNutrients['YEAR'] == 1982]\n",
    "\n",
    "# Get foliar nutrient levels 1985\n",
    "foliarNutrients85 = foliarNutrients.loc[foliarNutrients['YEAR'] == 1985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add foliar nutrients to tree data \n",
    "allTreeData82_85 = pd.merge(clean82_85, foliarNutrients82, on=\"PLOT\", how=\"outer\")\n",
    "allTreeData85_90 = pd.merge(clean85_90, foliarNutrients85, on=\"PLOT\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting percent growth for each tree per year\n",
    "# perc_growth = (D85-D82)/D82\n",
    "allTreeData82_85[\"Growth\"] = (allTreeData82_85[\"D85\"]-allTreeData82_85[\"D82\"])/allTreeData82_85[\"D82\"]\n",
    "# growth_per_yr = prec_growth/85-82\n",
    "allTreeData82_85[\"Growth/yr\"] = allTreeData82_85[\"Growth\"]/3\n",
    "\n",
    "#getting percent growth for 1985-1990\n",
    "# perc_growth = (D90-D85)/D825\n",
    "allTreeData85_90[\"Growth\"] = (allTreeData85_90[\"D90\"]-allTreeData85_90[\"D85\"])/allTreeData85_90[\"D85\"]\n",
    "# growth_per_yr = prec_growth/85-82\n",
    "allTreeData85_90[\"Growth/yr\"] = allTreeData85_90[\"Growth\"]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns: \n",
    "relColumns = [\"SPECIES\", \"TOTN\", \"TOTP\", \"TOTK\", \"TOTS\", \"TOTCa\", \"Growth/yr\"]\n",
    "modelingData82 = allTreeData82_85[relColumns]\n",
    "modelingData85 = allTreeData85_90[relColumns]\n",
    "\n",
    "# combine dataframes to make one dataframe with all data needed\n",
    "modelingData = pd.concat([modelingData82, modelingData85], ignore_index=True)\n",
    "modelingData['id'] = modelingData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='bins'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimental data analysis looking at distribution and possible binning\n",
    "import matplotlib\n",
    "\n",
    "# creating a copy of current modeling data\n",
    "modelingCopy = modelingData\n",
    "\n",
    "# Creating Histogram for percent growth per year\n",
    "bins = [0, .05, .1, .2, 50]\n",
    "modelingCopy['bins'] = pd.cut(modelingData['Growth/yr'], bins = bins, include_lowest=True)\n",
    "plotDF = modelingCopy.groupby('bins').bins.count()\n",
    "plotDF.plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Columns for neural network classification\n",
    "# What bin does the growth/yr fall into? that bin gets a 1, others get 0\n",
    "\n",
    "mergeDF = pd.DataFrame(columns=['id', 'bin'])\n",
    "\n",
    "# For each row in modelingData setting a bin based off how growth/year\n",
    "for index, row in modelingData.iterrows():\n",
    "    #bins: 0-0.05, 0.05+ to 0.1, 0.1+ to 0.2, 0.2+\n",
    "    if row['Growth/yr'] <= .05:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 0})\n",
    "    elif row['Growth/yr'] <= .1:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 1})\n",
    "    elif row['Growth/yr'] <= .2:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 2})\n",
    "    else:\n",
    "        mergeDF.loc[index] = pd.Series({'id': index, 'bin': 3})\n",
    "\n",
    "# joining data to include bin\n",
    "modelingData = pd.merge(modelingData, mergeDF, on='id', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-3f59790ad1d2>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  classifyData['SPECIES'] = speciesNumList\n"
     ]
    }
   ],
   "source": [
    "# Getting data ready for the Classification Neural Network\n",
    "\n",
    "# Input variables: SPECIES, TOTN, TOTP, TOTK, TOTS, TOTCa, and TOTMg (not used)\n",
    "# label: fixed bins (1 for correct bin, 0 for all others)\n",
    "classifyColumns = ['SPECIES', 'TOTN', 'TOTP', 'TOTK', 'TOTS', 'TOTCa', 'bin']\n",
    "classifyData = modelingData[classifyColumns]\n",
    "\n",
    "# for turning 'species' column into numerical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "speciesLabEncoder = LabelEncoder()\n",
    "\n",
    "# Transforms species into numerical data\n",
    "speciesNumList = speciesLabEncoder.fit_transform(classifyData['SPECIES'])\n",
    "# code for reverting: species = le.inverse_transform(label)\n",
    "# changing the species column to numerical values\n",
    "classifyData['SPECIES'] = speciesNumList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into input and output data\n",
    "classifyX = classifyData.iloc[:,:6].values\n",
    "classifyY = classifyData.iloc[:,6:7].values\n",
    "\n",
    "# Transforming output into binary values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "classifyY = ohe.fit_transform(classifyY).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "classifyX_train, classifyX_test, classifyY_train, classifyY_test = train_test_split(classifyX, classifyY, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building and Compiling Neural Network for Classification\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifyModel = Sequential()\n",
    "classifyModel.add(Dense(24, input_dim=6, activation='relu'))\n",
    "classifyModel.add(Dense(24, activation='relu'))\n",
    "#trying to overfit\n",
    "classifyModel.add(Dense(36, activation='relu'))\n",
    "classifyModel.add(Dense(36, activation='relu'))\n",
    "classifyModel.add(Dense(24, activation='relu'))\n",
    "classifyModel.add(Dense(12, activation='relu'))\n",
    "classifyModel.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "classifyModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 2s 2ms/step - loss: 1.3282 - accuracy: 0.4311\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.1120 - accuracy: 0.4983\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0628 - accuracy: 0.5002\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1.0326 - accuracy: 0.5192\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0210 - accuracy: 0.5301\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0178 - accuracy: 0.5154\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0086 - accuracy: 0.5251\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 1.0062 - accuracy: 0.5239\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - ETA: 0s - loss: 1.0110 - accuracy: 0.53 - 0s 2ms/step - loss: 1.0105 - accuracy: 0.5332\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0097 - accuracy: 0.5318\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0083 - accuracy: 0.5276\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 1.0079 - accuracy: 0.5220\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9951 - accuracy: 0.5286\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0004 - accuracy: 0.5238: 0s - loss: 1\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 1s 6ms/step - loss: 1.0083 - accuracy: 0.5159: 0s - loss: - ETA: 0s - loss: 1.0092 - accuracy: \n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9919 - accuracy: 0.5284\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 1.0038 - accuracy: 0.5198: 0s - loss: 1.0053 - accura\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 1s 6ms/step - loss: 1.0064 - accuracy: 0.5164\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 1s 5ms/step - loss: 0.9894 - accuracy: 0.5373: 0s - loss: 0.9846 - accuracy: 0.53 - ETA: 0s - loss: 0.9850 \n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 1.0008 - accuracy: 0.5233\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9909 - accuracy: 0.5272\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9948 - accuracy: 0.5288\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9823 - accuracy: 0.5391\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9790 - accuracy: 0.5399\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9937 - accuracy: 0.5300\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9982 - accuracy: 0.5256\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/step - loss: 0.9866 - accuracy: 0.5396\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9925 - accuracy: 0.5352\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 1s 4ms/step - loss: 0.9959 - accuracy: 0.5251\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9892 - accuracy: 0.5340\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9759 - accuracy: 0.5447\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9835 - accuracy: 0.5437\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9843 - accuracy: 0.5469\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9940 - accuracy: 0.5328\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9697 - accuracy: 0.5523\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5397\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9762 - accuracy: 0.5494\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9831 - accuracy: 0.5479\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.5467\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9679 - accuracy: 0.5495\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9712 - accuracy: 0.5562\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9673 - accuracy: 0.5529\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.5532\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9687 - accuracy: 0.5573\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9833 - accuracy: 0.5485\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9626 - accuracy: 0.5532\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9693 - accuracy: 0.5534\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9586 - accuracy: 0.5612\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9715 - accuracy: 0.5529\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9640 - accuracy: 0.5534\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9701 - accuracy: 0.5553\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9470 - accuracy: 0.5593\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9694 - accuracy: 0.5638\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9569 - accuracy: 0.5620\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9566 - accuracy: 0.5633\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9538 - accuracy: 0.5649\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.9594 - accuracy: 0.5595\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9569 - accuracy: 0.5686\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9465 - accuracy: 0.5698\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5594\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9730 - accuracy: 0.5615\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9659 - accuracy: 0.5625\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9622 - accuracy: 0.5655\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9566 - accuracy: 0.5659\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.5669\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9611 - accuracy: 0.5635\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9501 - accuracy: 0.5716\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9609 - accuracy: 0.5695\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9600 - accuracy: 0.5637\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9545 - accuracy: 0.5709\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9663 - accuracy: 0.5602\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9543 - accuracy: 0.5654\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9638 - accuracy: 0.5681\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9514 - accuracy: 0.5682\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.5669\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9550 - accuracy: 0.5596\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9590 - accuracy: 0.5649\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9469 - accuracy: 0.5710\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9446 - accuracy: 0.5765\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9578 - accuracy: 0.5688\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9562 - accuracy: 0.5684\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9520 - accuracy: 0.5696\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9450 - accuracy: 0.5685\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9488 - accuracy: 0.5737\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9435 - accuracy: 0.5688\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9483 - accuracy: 0.5754\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9658 - accuracy: 0.5564\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9458 - accuracy: 0.5700\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9497 - accuracy: 0.5723\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9498 - accuracy: 0.5685\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9432 - accuracy: 0.5729\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9551 - accuracy: 0.5751\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9682 - accuracy: 0.5605\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9519 - accuracy: 0.5722\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9521 - accuracy: 0.5663\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9574 - accuracy: 0.5674\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9383 - accuracy: 0.5830\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9469 - accuracy: 0.5655\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9409 - accuracy: 0.5671\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/step - loss: 0.9554 - accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "#Training classification model\n",
    "training = classifyModel.fit(classifyX_train, classifyY_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing classification model\n",
    "import numpy as np\n",
    "prediction = classifyModel.predict(classifyX_test)\n",
    "\n",
    "# undoing onehot encoder\n",
    "pred = list()\n",
    "for i in range(len(prediction)):\n",
    "    pred.append(np.argmax(prediction[i]))\n",
    "test = list()\n",
    "for i in range(len(classifyY_test)):\n",
    "    test.append(np.argmax(classifyY_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.5727190605239386\n"
     ]
    }
   ],
   "source": [
    "# Determining accuracy of Classification Neural Network model\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifyAccuracy = accuracy_score(pred,test)\n",
    "print('Accuracy is:', classifyAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Columns for neural network regression\n",
    "# input variables: SPECIES, TOTN, TOTP, TOTK, TOTS, TOTCa, and TOTMg (not used)\n",
    "# label: predicting growth per year"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
